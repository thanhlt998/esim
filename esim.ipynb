{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.callbacks import SaveModelCallback, EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS = '<UNK>', '<PAD>', '<BOS>', '<EOS>'\n",
    "LABELS = ['entailment', 'neutral', 'contradiction']\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class WordVocab(Vocab):\n",
    "    def numericalize(self, t:Collection[str], lowercase=True) -> List[int]:\n",
    "        return [self.stoi[self.process_word(w, lowercase)] for w in t]\n",
    "\n",
    "    @staticmethod\n",
    "    def process_word(word: str, lowercase=True):\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        return word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_pretrained_vocab_embedding(type: str, path, is_zip=True, inside_zip_fn=None):\n",
    "    def load_pretrained_embedding_from_fasttext_cc_model(path):\n",
    "        model = FastText.load_model(path)\n",
    "\n",
    "        embedding_dim = model.get_dimension()\n",
    "        words = [UNK, PAD, *model.words, BOS, EOS]\n",
    "        vocab = WordVocab(words)\n",
    "        embeddings = nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim,dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim,dtype=torch.float),\n",
    "                torch.tensor(model.get_input_matrix()),\n",
    "                (torch.rand(2, embedding_dim,dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "\n",
    "    def load_pretrained_embedding_from_gensim_fasttext_model(path):\n",
    "        model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "        embedding_dim = model.vector_size\n",
    "        words = [UNK, PAD, *model.index2word, BOS, EOS]\n",
    "        vocab = WordVocab(words)\n",
    "        embeddings = nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim, dtype=torch.float)\n",
    "                                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim, dtype=torch.float),\n",
    "                torch.tensor(model.vectors),\n",
    "                (torch.rand(2, embedding_dim, dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "    def load_glove_embedding(fn: str, is_zip=True, inside_zip_fn=None):\n",
    "        assert is_zip and inside_zip_fn is not None, 'Must provide file name inside zip'\n",
    "\n",
    "        def load_from_buffer_with_pandas(buffer):\n",
    "            df = pd.read_csv(buffer, sep=' ', header=None, quoting=csv.QUOTE_NONE)\n",
    "            vocab = df.iloc[:, 0].values\n",
    "            vectors = df.iloc[:, 1:].values\n",
    "            return vocab, vectors\n",
    "\n",
    "        if is_zip:\n",
    "            with zipfile.ZipFile(fn) as zf:\n",
    "                with zf.open(inside_zip_fn) as f:\n",
    "                    vocab, vectors = load_from_buffer_with_pandas(f)\n",
    "                    f.close()\n",
    "                zf.close()\n",
    "        else:\n",
    "            with open(fn, mode='r', encoding='utf8') as f:\n",
    "                vocab, vectors = load_from_buffer_with_pandas(f)\n",
    "\n",
    "        vocab = WordVocab([UNK, PAD, *vocab, BOS, EOS])\n",
    "        embedding_dim = vectors.shape[1]\n",
    "        embeddings =  nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim, dtype=torch.float)\n",
    "                                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim, dtype=torch.float),\n",
    "                torch.tensor(vectors, dtype=torch.float),\n",
    "                (torch.rand(2, embedding_dim, dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "    assert type in ['fasttext_cc', 'fasttext_gensim', 'glove']\n",
    "    if type == 'fasttext_cc':\n",
    "        return load_pretrained_embedding_from_fasttext_cc_model(path)\n",
    "    elif type == 'fasttext_cc':\n",
    "        return load_pretrained_embedding_from_gensim_fasttext_model(path)\n",
    "    else:\n",
    "        return load_glove_embedding(path, is_zip=is_zip, inside_zip_fn=inside_zip_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "path = Path('.')\n",
    "train = pd.read_csv('data/csv/train.csv', sep='\\t'); train.fillna('', inplace=True)\n",
    "test = pd.read_csv('data/csv/test.csv', sep='\\t'); test.fillna('', inplace=True)\n",
    "dev = pd.read_csv('data/csv/dev.csv', sep='\\t'); dev.fillna('', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PairPreProcessor(PreProcessor):\n",
    "    def __init__(self, vocab: WordVocab, tokenizer: BaseTokenizer=None, ds: Collection=None):\n",
    "        super(PairPreProcessor, self).__init__(ds=ds)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = Tokenizer(post_rules=[], pre_rules=[])\n",
    "        self.tok = tokenizer if tokenizer else SpacyTokenizer('en')\n",
    "\n",
    "    def process_one(self, item:Any):\n",
    "        premise, hypothesis = item\n",
    "        premise_words, hypothesis_words = (\n",
    "            self.tokenizer.process_text(premise, self.tok),\n",
    "            self.tokenizer.process_text(hypothesis, self.tok)\n",
    "        )\n",
    "        premise_token_ids, hypothesis_token_ids = (\n",
    "            self.vocab.numericalize([BOS, *premise_words, EOS]), self.vocab.numericalize([BOS, *hypothesis_words, EOS])\n",
    "        )\n",
    "        return premise_token_ids, hypothesis_token_ids\n",
    "\n",
    "class LabelPreProcessor(PreProcessor):\n",
    "    def __init__(self, vocab: Vocab):\n",
    "        super(LabelPreProcessor, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def process_one(self, item:Any):\n",
    "        return self.vocab.stoi[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "word_vocab, emb = load_pretrained_vocab_embedding('glove', 'data/pretrained/glove.6B.zip', is_zip=True, inside_zip_fn='glove.6B.300d.txt')\n",
    "label_vocab = Vocab(LABELS)\n",
    "\n",
    "pair_processor = PairPreProcessor(word_vocab)\n",
    "label_processor = LabelPreProcessor(vocab=label_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_il = ItemList(\n",
    "    items=train[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()\n",
    "\n",
    "dev_il = ItemList(\n",
    "    items=dev[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()\n",
    "\n",
    "test_il = ItemList(\n",
    "    items=test[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_tl = ItemList(\n",
    "    items=train['label'].values,\n",
    "    processor=label_processor\n",
    ").process()\n",
    "\n",
    "dev_tl = ItemList(\n",
    "    items=dev['label'].values,\n",
    "    processor=label_processor\n",
    ").process()\n",
    "\n",
    "test_tl = ItemList(\n",
    "    items=test['label'].values,\n",
    "    processor=label_processor\n",
    ").process()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_ll = LabelList(x=train_il, y=train_tl)\n",
    "dev_ll = LabelList(x=dev_il, y=dev_tl)\n",
    "test_ll = LabelList(x=test_il, y=test_tl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_vocab: WordVocab,\n",
    "            label_vocab: Vocab,\n",
    "            pad_first=False\n",
    "    ):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.pad_first = pad_first\n",
    "\n",
    "        self.word_pad_idx = self.word_vocab.stoi[PAD]\n",
    "\n",
    "    @staticmethod\n",
    "    def padding_collate(samples, pad_idx=1, pad_first=False, sort=False):\n",
    "        lengths = [len(s) for s in samples]\n",
    "        max_len = max(lengths)\n",
    "        res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "        for i,s in enumerate(samples):\n",
    "            if pad_first: res[i, -len(s):] = LongTensor(s)\n",
    "            else:         res[i, :len(s) ] = LongTensor(s)\n",
    "\n",
    "        if sort:\n",
    "            args_sort = torch.tensor(lengths, dtype=torch.long).argsort(descending=True)\n",
    "            recover_idxs = torch.argsort(args_sort)\n",
    "            return res, args_sort, recover_idxs\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __call__(self, batch_data):\n",
    "        batch_data_sort_by_promises = sorted(batch_data, key=lambda x: len(x[0][0]), reverse=True)\n",
    "        xs, ys = zip(*batch_data_sort_by_promises)\n",
    "\n",
    "        promises, hypothesises = zip(*xs)\n",
    "\n",
    "        # pad promises, hypothesises\n",
    "        pad_promises, pad_hypothesises = (\n",
    "            self.padding_collate(promises, pad_idx=self.word_pad_idx, pad_first=self.pad_first),\n",
    "            self.padding_collate(hypothesises, pad_idx=self.word_pad_idx, pad_first=self.pad_first, sort=True)\n",
    "        )\n",
    "\n",
    "        # convert ys to tensor\n",
    "        ys = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "        return (pad_promises, pad_hypothesises), ys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "bs = 32\n",
    "databunch = DataBunch.create(\n",
    "    train_ds=train_ll,\n",
    "    valid_ds=dev_ll,\n",
    "    test_ds=test_ll,\n",
    "    collate_fn=CollateFn(word_vocab=word_vocab, label_vocab=label_vocab),\n",
    "    bs=bs,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# (premise, (hypothesis, hypothesis_args_sort, hypothesis_recover_idxs)), labels = databunch.one_batch()\n",
    "#\n",
    "# premise_mask, hypothesis_mask = premise != word_vocab.stoi[PAD], hypothesis != word_vocab.stoi[PAD]\n",
    "# premise_lengths, hypothesis_lengths = torch.sum(premise_mask, dim=1), torch.sum(hypothesis_mask, dim=1)\n",
    "# inputs = (emb(premise), premise_lengths), (emb(hypothesis), hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n",
    "# premise.size(), hypothesis.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size(), labels.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0., batch_first=True):\n",
    "        super(RNNDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.:\n",
    "            return x\n",
    "        if self.batch_first:\n",
    "            return self.dropout_mask(x, (x.size(0), 1, x.size(2)), p=self.p) * x\n",
    "        else:\n",
    "            return self.dropout_mask(x, (1, x.size(1), x.size(2)), p=self.p) * x\n",
    "\n",
    "    @staticmethod\n",
    "    def dropout_mask(x, sizes, p):\n",
    "        return x.new(*sizes).bernoulli_(1-p).div_(1-p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class InputEncoding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size=300,\n",
    "            hidden_size=300,\n",
    "            bidirectional=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.premise_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "\n",
    "        self.hypothesis_lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "\n",
    "        self.premise_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "        self.hypothesis_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        (premise_batch, premise_lengths), (hypothesis_batch, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        # sort hypothesis with lengths\n",
    "        hypothesis_batch = hypothesis_batch[hypothesis_args_sort]\n",
    "        hypothesis_lengths = hypothesis_lengths[hypothesis_args_sort]\n",
    "\n",
    "        packed_premise_batch = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.premise_dropout(premise_batch),\n",
    "            lengths=premise_lengths,\n",
    "            batch_first=True\n",
    "        )\n",
    "        packed_hypothesis_batch = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.hypothesis_dropout(hypothesis_batch),\n",
    "            lengths=hypothesis_lengths,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        packed_premise_output, _ = self.premise_lstm(packed_premise_batch)\n",
    "        packed_hypothesis_output, _ = self.hypothesis_lstm(packed_hypothesis_batch)\n",
    "\n",
    "        premise_output, _ = nn.utils.rnn.pad_packed_sequence(packed_premise_output)\n",
    "        hypothesis_output, _ = nn.utils.rnn.pad_packed_sequence(packed_hypothesis_output)\n",
    "\n",
    "        # recover hypothesis\n",
    "        hypothesis_output = hypothesis_output[:, hypothesis_recover_idxs, :]\n",
    "        hypothesis_lengths = hypothesis_lengths[hypothesis_recover_idxs]\n",
    "\n",
    "        return (premise_output, premise_lengths), (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# input_encoding = InputEncoding(\n",
    "#     input_size=300,\n",
    "#     hidden_size=300,\n",
    "#     p_dropout=0.5\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# ((premise_output, premise_lengths),\n",
    "#  (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)) = input_encoding(inputs)\n",
    "# print(premise_output.size(), premise_lengths.size())\n",
    "# print(hypothesis_output.size(), hypothesis_lengths.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "class LocalInference(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalInference, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (premise_dash, premise_lengths), (hypothesis_dash, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        premise_dash = premise_dash.transpose(1, 0)\n",
    "        hypothesis_dash = hypothesis_dash.transpose(1, 0)\n",
    "\n",
    "        attention = torch.bmm(premise_dash, hypothesis_dash.transpose(2, 1))\n",
    "        mask = torch.zeros(attention.size(), device=attention.device)\n",
    "        for i in range(premise_lengths.size(0)):\n",
    "            mask[i, :premise_lengths[i], :hypothesis_lengths[i]] = 1\n",
    "\n",
    "        attention_exp = torch.exp(attention * mask + (1 - mask) * (-1e1))\n",
    "\n",
    "        attention_normalize_premise = attention_exp / torch.sum(attention_exp, dim=2, keepdim=True) * mask\n",
    "        attention_normalize_hypothesis = attention_exp / torch.sum(attention_exp, dim=1, keepdim=True) * mask\n",
    "\n",
    "        premise_tilde = torch.bmm(attention_normalize_premise, hypothesis_dash)\n",
    "        hypothesis_tilde = torch.bmm(attention_normalize_hypothesis.transpose(1, 2), premise_dash)\n",
    "\n",
    "        premise_m, hypothesis_m = map(lambda x: torch.cat([x[0], x[1], x[0] - x[1], x[0] * x[1]], dim=2),\n",
    "                                      [(premise_dash, premise_tilde), (hypothesis_dash, hypothesis_tilde)])\n",
    "\n",
    "        return (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# local_inference = LocalInference()\n",
    "# premise_dash = (premise_output, premise_lengths)\n",
    "# hypothesis_dash = (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n",
    "# print(hypothesis_lengths[hypothesis_args_sort])\n",
    "# (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_arg_sort, hypothesis_recover_idxs) = local_inference(\n",
    "#     (premise_dash, hypothesis_dash)\n",
    "# )\n",
    "# premise_m.size(), hypothesis_m.size(), premise_lengths.size(), hypothesis_lengths.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            out_features,\n",
    "            bias,\n",
    "    ):\n",
    "        super(Projection, self).__init__()\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "        premise_m = self.relu(self.projection(premise_m))\n",
    "        hypothesis_m = self.relu(self.projection(hypothesis_m))\n",
    "        return (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "class InferenceComposition(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size=300,\n",
    "            bidirectional=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        super(InferenceComposition, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.premise_inference_composition = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.hypothesis_inference_composition = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.premise_inference_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "        self.hypothesis_inference_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        # sort hypothesis with length\n",
    "        hypothesis_m = hypothesis_m[hypothesis_args_sort]\n",
    "        hypothesis_lengths = hypothesis_lengths[hypothesis_args_sort]\n",
    "\n",
    "        packed_premise_m = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.premise_inference_dropout(premise_m),\n",
    "            lengths=premise_lengths,\n",
    "            batch_first=True\n",
    "        )\n",
    "        packed_hypothesis_m = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.hypothesis_inference_dropout(hypothesis_m),\n",
    "            lengths=hypothesis_lengths,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        packed_premise_v, _ = self.premise_inference_composition(packed_premise_m)\n",
    "        packed_hypothesis_v, _ = self.hypothesis_inference_composition(packed_hypothesis_m)\n",
    "\n",
    "        premise_v, _ = nn.utils.rnn.pad_packed_sequence(packed_premise_v)\n",
    "        hypothesis_v, _ = nn.utils.rnn.pad_packed_sequence(packed_hypothesis_v)\n",
    "\n",
    "        premise_v_max_pooling, _ = torch.max(premise_v, dim=0)\n",
    "        hypothesis_v_max_pooling, _ = torch.max(hypothesis_v, dim=0)\n",
    "        premise_v_avg_pooling = torch.sum(premise_v, dim=0) / premise_v.size(0)\n",
    "        hypothesis_v_avg_pooling = torch.sum(hypothesis_v, dim=0) / hypothesis_v.size(0)\n",
    "\n",
    "        return torch.cat([\n",
    "            premise_v_max_pooling,\n",
    "            premise_v_avg_pooling,\n",
    "            hypothesis_v_max_pooling[hypothesis_recover_idxs],\n",
    "            hypothesis_v_avg_pooling[hypothesis_recover_idxs],\n",
    "        ], dim=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# inference_composition = InferenceComposition(\n",
    "#     input_size=4 * 300 * 2,\n",
    "#     hidden_size=300,\n",
    "#     bidirectional=True,\n",
    "#     p_dropout=0.\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# composition = inference_composition(\n",
    "#     ((premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs))\n",
    "# )\n",
    "# composition.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            num_classes,\n",
    "            hidden_sizes: list=None,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0,\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_classes = num_classes\n",
    "        self.act_func = act_func\n",
    "        self.bias = bias\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mlp = self.get_mlp(\n",
    "            input_size=input_size,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_classes=num_classes,\n",
    "            act_func=act_func,\n",
    "            bias=bias,\n",
    "            p_dropout=p_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.mlp(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mlp(input_size, hidden_sizes, num_classes, act_func, bias=True, p_dropout=0.):\n",
    "        assert num_classes > 1, 'number of classes must be more than one'\n",
    "        assert act_func in ['sigmoid', 'relu', 'tanh']\n",
    "        map_act_func = {\n",
    "            'sigmoid': nn.Sigmoid,\n",
    "            'relu': nn.ReLU,\n",
    "            'tanh': nn.Tanh,\n",
    "        }\n",
    "        sizes = [input_size, *hidden_sizes, num_classes]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            if i != len(sizes) - 2:\n",
    "                layers.extend([\n",
    "                    nn.Linear(\n",
    "                        in_features=sizes[i],\n",
    "                        out_features=sizes[i + 1],\n",
    "                        bias=bias\n",
    "                    ),\n",
    "                    map_act_func[act_func](),\n",
    "                    nn.Dropout(p=p_dropout)\n",
    "                ])\n",
    "            else:\n",
    "                layers.append(nn.Linear(\n",
    "                        in_features=sizes[i],\n",
    "                        out_features=sizes[i + 1],\n",
    "                        bias=bias\n",
    "                    ))\n",
    "        return nn.Sequential(*layers)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# classifier = Classifier(\n",
    "#     input_size=4 * 300 * 2,\n",
    "#     num_classes=len(label_vocab.itos),\n",
    "#     hidden_sizes=[512, 256],\n",
    "#     act_func='sigmoid',\n",
    "#     bias=True\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# output = classifier(composition)\n",
    "# output.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# loss = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# loss(output, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_vocab: WordVocab,\n",
    "            embedding_dim,\n",
    "            embeddings: nn.Embedding,\n",
    "            hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes: list,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0.,\n",
    "    ):\n",
    "        super(ESIM, self).__init__()\n",
    "        assert embedding_dim is not None or embeddings is not None, 'embedding_dim and embeddings cannot both be none'\n",
    "        self.word_vocab = word_vocab\n",
    "        self.pad_idx = word_vocab.stoi[PAD]\n",
    "        self.embedding = embeddings if embeddings is not None else nn.Embedding(\n",
    "            len(self.word_vocab.itos),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=self.pad_idx\n",
    "        )\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "\n",
    "        self.esim = self.build_model(\n",
    "            embedding_dim,\n",
    "            hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes,\n",
    "            act_func,\n",
    "            bias,\n",
    "            p_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, *inputs):\n",
    "        premise_batch, (hypothesis_batch, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "        premise_mask, hypothesis_mask = premise_batch != self.pad_idx, hypothesis_batch != self.pad_idx\n",
    "        premise_lengths, hypothesis_lengths = torch.sum(premise_mask, dim=1), torch.sum(hypothesis_mask, dim=1)\n",
    "        inputs = (self.embedding(premise_batch), premise_lengths), (self.embedding(hypothesis_batch), hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n",
    "        return self.esim(inputs)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model(\n",
    "            input_size,\n",
    "            hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes: list,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            InputEncoding(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                p_dropout=p_dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(LocalInference())\n",
    "\n",
    "        output_local_inference_dim = (2 if bidirectional else 1) * hidden_size * 4\n",
    "\n",
    "        layers.append(\n",
    "            Projection(\n",
    "                in_features=output_local_inference_dim,\n",
    "                out_features=hidden_size,\n",
    "                bias=bias\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            InferenceComposition(\n",
    "                input_size=hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                bidirectional=bidirectional,\n",
    "                p_dropout=p_dropout\n",
    "            )\n",
    "        )\n",
    "        output_inference_composition_dim = (2 if bidirectional else 1) * hidden_size * 4\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Dropout(p=p_dropout),\n",
    "                Classifier(\n",
    "                    input_size=output_inference_composition_dim,\n",
    "                    hidden_sizes=hidden_sizes,\n",
    "                    num_classes=num_classes,\n",
    "                    act_func=act_func,\n",
    "                    bias=bias\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__( self, *args, **kwargs):\n",
    "        for k, v in self.default_config().items():\n",
    "            self.__setattr__(k, v)\n",
    "        for k, v in kwargs.items():\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "        assert self.__getattribute__('word_vocab') is not None\n",
    "        assert self.__getattribute__('num_classes') is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def default_config():\n",
    "        hidden_size = 300\n",
    "        return {\n",
    "            'word_vocab': None,\n",
    "            'embedding_dim': 300,\n",
    "            'embeddings': None,\n",
    "            'hidden_size': hidden_size,\n",
    "            'bidirectional': True,\n",
    "            'num_classes': None,\n",
    "            'hidden_sizes': [hidden_size],\n",
    "            'act_func': 'tanh',\n",
    "            'bias': True,\n",
    "            'p_dropout': 0.5,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    word_vocab=word_vocab,\n",
    "    num_classes=len(label_vocab.itos)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhlt998/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = ESIM(\n",
    "    word_vocab=config.word_vocab,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    embeddings=config.embeddings,\n",
    "    hidden_size=config.hidden_size,\n",
    "    bidirectional=config.bidirectional,\n",
    "    num_classes=config.num_classes,\n",
    "    hidden_sizes=config.hidden_sizes,\n",
    "    act_func=config.act_func,\n",
    "    bias=config.bias,\n",
    "    p_dropout=config.p_dropout\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# output = model(premise, (hypothesis, hypothesis_args_sort, hypothesis_recover_idxs))\n",
    "# output.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "    data=databunch,\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    path='.',\n",
    "    metrics=[accuracy]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "layers = [\n",
    "    learner.model.embedding,\n",
    "    learner.model.esim[0],\n",
    "    learner.model.esim[2],\n",
    "    learner.model.esim[3],\n",
    "    learner.model.esim[5]\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "Learner(data=DataBunch;\n\nTrain: LabelList (550152 items)\nx: ItemList\n[list([0, 9, 901, 15, 9, 2869, 11072, 76, 9, 2323, 137, 7352, 4, 0])\n list([0, 9, 901, 16, 790, 28, 2869, 12, 9, 993, 4, 0])],[list([0, 9, 901, 15, 9, 2869, 11072, 76, 9, 2323, 137, 7352, 4, 0])\n list([0, 9, 901, 16, 24, 9, 19303, 3, 7489, 31, 119030, 4, 0])],[list([0, 9, 901, 15, 9, 2869, 11072, 76, 9, 2323, 137, 7352, 4, 0]) list([0, 9, 901, 16, 13079, 3, 15, 9, 2869, 4, 0])],[list([0, 273, 8783, 7, 8887, 24, 3536, 0]) list([0, 41, 34, 8783, 24, 46, 1110, 0])],[list([0, 273, 8783, 7, 8887, 24, 3536, 0]) list([0, 65, 34, 273, 945, 0])]\ny: ItemList\n1,2,0,1,0\nPath: .;\n\nValid: LabelList (10000 items)\nx: ItemList\n[list([0, 57, 268, 34, 14720, 112, 1385, 6, 244, 6857, 4, 0])\n list([0, 2, 4733, 34, 22231, 10928, 112, 1385, 6, 244, 6857, 51, 122, 4527, 4155, 4, 0])],[list([0, 57, 268, 34, 14720, 112, 1385, 6, 244, 6857, 4, 0]) list([0, 57, 789, 34, 1385, 6857, 4, 0])],[list([0, 57, 268, 34, 14720, 112, 1385, 6, 244, 6857, 4, 0]) list([0, 2, 303, 34, 800, 589, 9, 25406, 4, 0])],[list([0, 57, 463, 273, 8, 1187, 19509, 3, 50, 19, 2, 225, 792, 7, 50, 19, 2, 225, 234, 34, 1769, 15, 4838, 2013, 8, 9, 9517, 7, 11736, 46, 1283, 8, 9, 9243, 4, 0])\n list([0, 57, 1815, 8, 9732, 19509, 9036, 46, 1283, 4, 0])],[list([0, 57, 463, 273, 8, 1187, 19509, 3, 50, 19, 2, 225, 792, 7, 50, 19, 2, 225, 234, 34, 1769, 15, 4838, 2013, 8, 9, 9517, 7, 11736, 46, 1283, 8, 9, 9243, 4, 0])\n list([0, 57, 1815, 24, 9, 30688, 9036, 46, 1283, 4, 0])]\ny: ItemList\n1,0,2,0,1\nPath: .;\n\nTest: LabelList (10000 items)\nx: ItemList\n[list([0, 39, 514, 8053, 10910, 6, 2, 9321, 21, 41, 5624, 25927, 1501, 27, 2, 541, 24, 9, 514, 4, 0])\n list([0, 2, 514, 33, 11430, 8, 2, 6479, 4, 0])],[list([0, 39, 514, 8053, 10910, 6, 2, 9321, 21, 41, 5624, 25927, 1501, 27, 2, 541, 24, 9, 514, 4, 0])\n list([0, 2, 514, 16, 2705, 19, 815, 4, 0])],[list([0, 39, 514, 8053, 10910, 6, 2, 9321, 21, 41, 5624, 25927, 1501, 27, 2, 541, 24, 9, 514, 4, 0])\n list([0, 9, 8053, 4101, 24, 9, 1446, 188, 4, 0])],[list([0, 9, 789, 19, 9, 991, 31023, 3, 1187, 5100, 7, 9, 193, 367, 18753, 4, 0]) list([0, 2, 789, 16, 463, 4, 0])],[list([0, 9, 789, 19, 9, 991, 31023, 3, 1187, 5100, 7, 9, 193, 367, 18753, 4, 0])\n list([0, 2, 789, 16, 193, 1753, 4, 0])]\ny: ItemList\n1,0,2,1,0\nPath: ., model=ESIM(\n  (embedding): Embedding(400004, 300, padding_idx=1)\n  (esim): Sequential(\n    (0): InputEncoding(\n      (premise_lstm): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n      (hypothesis_lstm): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n      (premise_dropout): RNNDropout()\n      (hypothesis_dropout): RNNDropout()\n    )\n    (1): LocalInference()\n    (2): Projection(\n      (projection): Linear(in_features=2400, out_features=300, bias=True)\n      (relu): ReLU()\n    )\n    (3): InferenceComposition(\n      (premise_inference_composition): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n      (hypothesis_inference_composition): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n      (premise_inference_dropout): RNNDropout()\n      (hypothesis_inference_dropout): RNNDropout()\n    )\n    (4): Dropout(p=0.5, inplace=False)\n    (5): Classifier(\n      (mlp): Sequential(\n        (0): Linear(in_features=2400, out_features=300, bias=True)\n        (1): Tanh()\n        (2): Dropout(p=0, inplace=False)\n        (3): Linear(in_features=300, out_features=3, bias=True)\n      )\n    )\n  )\n), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=CrossEntropyLoss(), metrics=[<function accuracy at 0x7f07d6015158>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('.'), model_dir='models', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[], layer_groups=[Sequential(\n  (0): Embedding(400004, 300, padding_idx=1)\n), Sequential(\n  (0): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n  (1): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n  (2): RNNDropout()\n  (3): RNNDropout()\n  (4): LocalInference()\n), Sequential(\n  (0): Linear(in_features=2400, out_features=300, bias=True)\n  (1): ReLU()\n), Sequential(\n  (0): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n  (1): LSTM(300, 300, batch_first=True, dropout=0.5, bidirectional=True)\n  (2): RNNDropout()\n  (3): RNNDropout()\n  (4): Dropout(p=0.5, inplace=False)\n), Sequential(\n  (0): Linear(in_features=2400, out_features=300, bias=True)\n  (1): Tanh()\n  (2): Dropout(p=0, inplace=False)\n  (3): Linear(in_features=300, out_features=3, bias=True)\n)], add_time=True, silent=False)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.split(layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (300) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-74ea54b65c71>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlearner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_find\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mlearner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/train.py\u001B[0m in \u001B[0;36mlr_find\u001B[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mcb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLRFinder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_it\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_div\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0mepochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mceil\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_it\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m     \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, epochs, lr, wd, callbacks)\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m         \u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcb\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_fns\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlistify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefaults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextra_callback_fns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlistify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m         \u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcreate_opt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mFloats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mFloats\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m->\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(epochs, learn, callbacks, metrics)\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0myb\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprogress_bar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpbar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m                 \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m                 \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_batch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mloss_batch\u001B[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001B[0m\n\u001B[1;32m     24\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_listy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mxb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mxb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mis_listy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0myb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0myb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0myb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m     \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mxb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m     \u001B[0mout\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_loss_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-20-6f3d8d257c4c>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, *inputs)\u001B[0m\n\u001B[1;32m     40\u001B[0m         \u001B[0mpremise_lengths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypothesis_lengths\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpremise_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msum\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhypothesis_mask\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdim\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         \u001B[0minputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpremise_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpremise_lengths\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0membedding\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhypothesis_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypothesis_lengths\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypothesis_args_sort\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhypothesis_recover_idxs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mesim\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mstaticmethod\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/container.py\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m     90\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmodule\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_modules\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m             \u001B[0minput\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodule\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-15-b78e6f062169>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         packed_premise_batch = nn.utils.rnn.pack_padded_sequence(\n\u001B[0;32m---> 42\u001B[0;31m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpremise_dropout\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpremise_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m             \u001B[0mlengths\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpremise_lengths\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m             \u001B[0mbatch_first\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m    539\u001B[0m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_slow_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    540\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 541\u001B[0;31m             \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mforward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    542\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mhook\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_forward_hooks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    543\u001B[0m             \u001B[0mhook_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-13-41f53a29b00f>\u001B[0m in \u001B[0;36mforward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m      9\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbatch_first\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdropout_mask\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mp\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m*\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: The size of tensor a (3) must match the size of tensor b (300) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.fit(\n",
    "    epochs=15,\n",
    "    callbacks=[\n",
    "      SaveModelCallback(learner, monitor='accuracy', every='improvement', name='best_model'),\n",
    "      EarlyStoppingCallback(learner, monitor='valid_loss', min_delta=0.01, patience=10),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "3496"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('best_model')\n",
    "learner.evaluate(dl=learner.data.test_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}