{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fastai.text import *\n",
    "from fastai.callbacks import SaveModelCallback, EarlyStoppingCallback\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fasttext import FastText\n",
    "from gensim.models import KeyedVectors\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "UNK, PAD, BOS, EOS = '<UNK>', '<PAD>', '<BOS>', '<EOS>'\n",
    "LABELS = ['entailment', 'neutral', 'contradiction']\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class WordVocab(Vocab):\n",
    "    def numericalize(self, t:Collection[str], lowercase=True) -> List[int]:\n",
    "        return [self.stoi[self.process_word(w, lowercase)] for w in t]\n",
    "\n",
    "    @staticmethod\n",
    "    def process_word(word: str, lowercase=True):\n",
    "        if lowercase:\n",
    "            word = word.lower()\n",
    "        return word"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def load_pretrained_vocab_embedding(type: str, path, is_zip=True, inside_zip_fn=None):\n",
    "    def load_pretrained_embedding_from_fasttext_cc_model(path):\n",
    "        model = FastText.load_model(path)\n",
    "\n",
    "        embedding_dim = model.get_dimension()\n",
    "        words = [UNK, PAD, *model.words, BOS, EOS]\n",
    "        vocab = WordVocab(words)\n",
    "        embeddings = nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim,dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim,dtype=torch.float),\n",
    "                torch.tensor(model.get_input_matrix()),\n",
    "                (torch.rand(2, embedding_dim,dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "\n",
    "    def load_pretrained_embedding_from_gensim_fasttext_model(path):\n",
    "        model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "        embedding_dim = model.vector_size\n",
    "        words = [UNK, PAD, *model.index2word, BOS, EOS]\n",
    "        vocab = WordVocab(words)\n",
    "        embeddings = nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim, dtype=torch.float)\n",
    "                                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim, dtype=torch.float),\n",
    "                torch.tensor(model.vectors),\n",
    "                (torch.rand(2, embedding_dim, dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "    def load_glove_embedding(fn: str, is_zip=True, inside_zip_fn=None):\n",
    "        assert is_zip and inside_zip_fn is not None, 'Must provide file name inside zip'\n",
    "\n",
    "        def load_from_buffer_with_pandas(buffer):\n",
    "            df = pd.read_csv(buffer, sep=' ', header=None, quoting=csv.QUOTE_NONE)\n",
    "            vocab = df.iloc[:, 0].values\n",
    "            vectors = df.iloc[:, 1:].values\n",
    "            return vocab, vectors\n",
    "\n",
    "        if is_zip:\n",
    "            with zipfile.ZipFile(fn) as zf:\n",
    "                with zf.open(inside_zip_fn) as f:\n",
    "                    vocab, vectors = load_from_buffer_with_pandas(f)\n",
    "                    f.close()\n",
    "                zf.close()\n",
    "        else:\n",
    "            with open(fn, mode='r', encoding='utf8') as f:\n",
    "                vocab, vectors = load_from_buffer_with_pandas(f)\n",
    "\n",
    "        vocab = WordVocab([UNK, PAD, *vocab, BOS, EOS])\n",
    "        embedding_dim = vectors.shape[1]\n",
    "        embeddings =  nn.Embedding.from_pretrained(\n",
    "            torch.cat([\n",
    "                (torch.rand(1, embedding_dim, dtype=torch.float)\n",
    "                                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim))),\n",
    "                torch.zeros(1, embedding_dim, dtype=torch.float),\n",
    "                torch.tensor(vectors, dtype=torch.float),\n",
    "                (torch.rand(2, embedding_dim, dtype=torch.float)\n",
    "                 .uniform_(- math.sqrt(3 / embedding_dim),math.sqrt(3 / embedding_dim)))\n",
    "            ]),\n",
    "            padding_idx=vocab.stoi[PAD]\n",
    "        )\n",
    "        return vocab, embeddings\n",
    "\n",
    "    assert type in ['fasttext_cc', 'fasttext_gensim', 'glove']\n",
    "    if type == 'fasttext_cc':\n",
    "        return load_pretrained_embedding_from_fasttext_cc_model(path)\n",
    "    elif type == 'fasttext_cc':\n",
    "        return load_pretrained_embedding_from_gensim_fasttext_model(path)\n",
    "    else:\n",
    "        return load_glove_embedding(path, is_zip=is_zip, inside_zip_fn=inside_zip_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "seed = 42\n",
    "path = Path('.')\n",
    "train = pd.read_csv('data/csv/train.csv', sep='\\t'); train.fillna('', inplace=True)\n",
    "test = pd.read_csv('data/csv/test.csv', sep='\\t'); test.fillna('', inplace=True)\n",
    "dev = pd.read_csv('data/csv/dev.csv', sep='\\t'); dev.fillna('', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "class PairPreProcessor(PreProcessor):\n",
    "    def __init__(self, vocab: WordVocab, tokenizer: BaseTokenizer=None, ds: Collection=None):\n",
    "        super(PairPreProcessor, self).__init__(ds=ds)\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = Tokenizer(post_rules=[], pre_rules=[])\n",
    "        self.tok = tokenizer if tokenizer else SpacyTokenizer('en')\n",
    "\n",
    "    def process_one(self, item:Any):\n",
    "        premise, hypothesis = item\n",
    "        premise_words, hypothesis_words = (\n",
    "            self.tokenizer.process_text(premise, self.tok),\n",
    "            self.tokenizer.process_text(hypothesis, self.tok)\n",
    "        )\n",
    "        premise_token_ids, hypothesis_token_ids = (\n",
    "            self.vocab.numericalize([BOS, *premise_words, EOS]), self.vocab.numericalize([BOS, *hypothesis_words, EOS])\n",
    "        )\n",
    "        return premise_token_ids, hypothesis_token_ids\n",
    "\n",
    "class LabelPreProcessor(PreProcessor):\n",
    "    def __init__(self, vocab: Vocab):\n",
    "        super(LabelPreProcessor, self).__init__()\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def process_one(self, item:Any):\n",
    "        return self.vocab.stoi[item]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "word_vocab, emb = load_pretrained_vocab_embedding('glove', 'data/pretrained/glove.6B.zip', is_zip=True, inside_zip_fn='glove.6B.300d.txt')\n",
    "label_vocab = Vocab(LABELS)\n",
    "\n",
    "pair_processor = PairPreProcessor(word_vocab)\n",
    "label_processor = LabelPreProcessor(vocab=label_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_il = ItemList(\n",
    "    items=train[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()\n",
    "\n",
    "dev_il = ItemList(\n",
    "    items=dev[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()\n",
    "\n",
    "test_il = ItemList(\n",
    "    items=test[['premise', 'hypothesis']].values,\n",
    "    processor=pair_processor\n",
    ").process()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_tl = ItemList(\n",
    "    items=train['label'].values,\n",
    "    processor=label_processor\n",
    ").process()\n",
    "\n",
    "dev_tl = ItemList(\n",
    "    items=dev['label'].values,\n",
    "    processor=label_processor\n",
    ").process()\n",
    "\n",
    "test_tl = ItemList(\n",
    "    items=test['label'].values,\n",
    "    processor=label_processor\n",
    ").process()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "train_ll = LabelList(x=train_il, y=train_tl)\n",
    "dev_ll = LabelList(x=dev_il, y=dev_tl)\n",
    "test_ll = LabelList(x=test_il, y=test_tl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_vocab: WordVocab,\n",
    "            label_vocab: Vocab,\n",
    "            pad_first=False\n",
    "    ):\n",
    "        self.word_vocab = word_vocab\n",
    "        self.label_vocab = label_vocab\n",
    "        self.pad_first = pad_first\n",
    "\n",
    "        self.word_pad_idx = self.word_vocab.stoi[PAD]\n",
    "\n",
    "    @staticmethod\n",
    "    def padding_collate(samples, pad_idx=1, pad_first=False, sort=False):\n",
    "        lengths = [len(s) for s in samples]\n",
    "        max_len = max(lengths)\n",
    "        res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "        for i,s in enumerate(samples):\n",
    "            if pad_first: res[i, -len(s):] = LongTensor(s)\n",
    "            else:         res[i, :len(s) ] = LongTensor(s)\n",
    "\n",
    "        if sort:\n",
    "            args_sort = torch.tensor(lengths, dtype=torch.long).argsort(descending=True)\n",
    "            recover_idxs = torch.argsort(args_sort)\n",
    "            return res, args_sort, recover_idxs\n",
    "\n",
    "        return res\n",
    "\n",
    "    def __call__(self, batch_data):\n",
    "        batch_data_sort_by_promises = sorted(batch_data, key=lambda x: len(x[0][0]), reverse=True)\n",
    "        xs, ys = zip(*batch_data_sort_by_promises)\n",
    "\n",
    "        promises, hypothesises = zip(*xs)\n",
    "\n",
    "        # pad promises, hypothesises\n",
    "        pad_promises, pad_hypothesises = (\n",
    "            self.padding_collate(promises, pad_idx=self.word_pad_idx, pad_first=self.pad_first),\n",
    "            self.padding_collate(hypothesises, pad_idx=self.word_pad_idx, pad_first=self.pad_first, sort=True)\n",
    "        )\n",
    "\n",
    "        # convert ys to tensor\n",
    "        ys = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "        return (pad_promises, pad_hypothesises), ys"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "bs = 3\n",
    "databunch = DataBunch.create(\n",
    "    train_ds=train_ll,\n",
    "    valid_ds=dev_ll,\n",
    "    test_ds=test_ll,\n",
    "    collate_fn=CollateFn(word_vocab=word_vocab, label_vocab=label_vocab),\n",
    "    bs=bs,\n",
    "    device=device\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([3, 20]),\n torch.Size([3, 10]),\n torch.Size([3]),\n torch.Size([3]),\n torch.Size([3]))"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(premise, (hypothesis, hypothesis_args_sort, hypothesis_recover_idxs)), labels = databunch.one_batch()\n",
    "premise.size(), hypothesis.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size(), labels.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0., batch_first=True):\n",
    "        super(RNNDropout, self).__init__()\n",
    "        self.p = p\n",
    "        self.batch_first = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0.:\n",
    "            return x\n",
    "        if self.batch_first:\n",
    "            return self.dropout_mask(x, (x.size(0), 1, x.size(2)), p=self.p) * x\n",
    "        else:\n",
    "            return self.dropout_mask(x, (1, x.size(1), x.size(2)), p=self.p) * x\n",
    "\n",
    "    @staticmethod\n",
    "    def dropout_mask(x, sizes, p):\n",
    "        return x.new(sizes).bernoulli_(1-p).div_(1-p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "class InputEncoding(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_vocab: WordVocab,\n",
    "            embedding_dim=300,\n",
    "            embeddings: nn.Embedding=None,\n",
    "            hidden_size=300,\n",
    "            bidirectional=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        super(InputEncoding, self).__init__()\n",
    "        assert embedding_dim is not None or embeddings is not None, 'embedding_dim and embeddings cannot both be none'\n",
    "        self.word_vocab = word_vocab\n",
    "        self.bidirectional = bidirectional\n",
    "        self.p_dropout = p_dropout\n",
    "        self.pad_idx = word_vocab.stoi[PAD]\n",
    "\n",
    "        self.embedding = embeddings if embeddings is not None else nn.Embedding(\n",
    "            len(self.word_vocab.itos),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=self.pad_idx\n",
    "        )\n",
    "        self.embedding_dim = self.embedding.embedding_dim\n",
    "        self.premise_lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "\n",
    "        self.hypothesis_lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout\n",
    "        )\n",
    "\n",
    "        self.premise_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "        self.hypothesis_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        premise_batch, (hypothesis_batch, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        # sort hypothesis with lengths\n",
    "        hypothesis_batch = hypothesis_batch[hypothesis_args_sort]\n",
    "\n",
    "        premise_mask, hypothesis_mask = premise_batch != self.pad_idx, hypothesis_batch != self.pad_idx\n",
    "        premise_lengths, hypothesis_lengths = torch.sum(premise_mask, dim=1), torch.sum(hypothesis_mask, dim=1)\n",
    "        packed_premise_batch = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.premise_dropout(self.embedding(premise_batch)),\n",
    "            lengths=premise_lengths,\n",
    "            batch_first=True\n",
    "        )\n",
    "        packed_hypothesis_batch = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.hypothesis_dropout(self.embedding(hypothesis_batch)),\n",
    "            lengths=hypothesis_lengths,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        packed_premise_output, _ = self.premise_lstm(packed_premise_batch)\n",
    "        packed_hypothesis_output, _ = self.hypothesis_lstm(packed_hypothesis_batch)\n",
    "\n",
    "        premise_output, _ = nn.utils.rnn.pad_packed_sequence(packed_premise_output)\n",
    "        hypothesis_output, _ = nn.utils.rnn.pad_packed_sequence(packed_hypothesis_output)\n",
    "\n",
    "        # recover hypothesis\n",
    "        hypothesis_output = hypothesis_output[:, hypothesis_recover_idxs, :]\n",
    "        hypothesis_lengths = hypothesis_lengths[hypothesis_recover_idxs]\n",
    "\n",
    "        return (premise_output, premise_lengths), (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "input_encoding = InputEncoding(\n",
    "    word_vocab=word_vocab,\n",
    "    embedding_dim=300,\n",
    "    embeddings=emb,\n",
    "    hidden_size=2,\n",
    "    p_dropout=0\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 4]) torch.Size([3])\n",
      "torch.Size([10, 3, 4]) torch.Size([3]) torch.Size([3]) torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "((premise_output, premise_lengths),\n",
    " (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)) = input_encoding((premise, (hypothesis, hypothesis_args_sort, hypothesis_recover_idxs)))\n",
    "print(premise_output.size(), premise_lengths.size())\n",
    "print(hypothesis_output.size(), hypothesis_lengths.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "class LocalInference(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LocalInference, self).__init__()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (premise_dash, premise_lengths), (hypothesis_dash, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        premise_dash = premise_dash.transpose(1, 0)\n",
    "        hypothesis_dash = hypothesis_dash.transpose(1, 0)\n",
    "\n",
    "        attention = torch.bmm(premise_dash, hypothesis_dash.transpose(2, 1))\n",
    "        mask = torch.zeros(attention.size(), device=attention.device)\n",
    "        for i in range(premise_lengths.size(0)):\n",
    "            mask[i, :premise_lengths[i], :hypothesis_lengths[i]] = 1\n",
    "\n",
    "        attention_exp = torch.exp(attention * mask + (1 - mask) * (-1e1))\n",
    "\n",
    "        attention_normalize_premise = attention_exp / torch.sum(attention_exp, dim=2, keepdim=True) * mask\n",
    "        attention_normalize_hypothesis = attention_exp / torch.sum(attention_exp, dim=1, keepdim=True) * mask\n",
    "\n",
    "        premise_tilde = torch.bmm(attention_normalize_premise, hypothesis_dash)\n",
    "        hypothesis_tilde = torch.bmm(attention_normalize_hypothesis.transpose(1, 2), premise_dash)\n",
    "\n",
    "        premise_m, hypothesis_m = map(lambda x: torch.cat([x[0], x[1], x[0] - x[1], x[0] * x[1]], dim=2),\n",
    "                                      [(premise_dash, premise_tilde), (hypothesis_dash, hypothesis_tilde)])\n",
    "\n",
    "        return (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10,  9,  9])\n"
     ]
    },
    {
     "data": {
      "text/plain": "(torch.Size([3, 20, 16]),\n torch.Size([3, 10, 16]),\n torch.Size([3]),\n torch.Size([3]),\n torch.Size([3]),\n torch.Size([3]))"
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_inference = LocalInference()\n",
    "premise_dash = (premise_output, premise_lengths)\n",
    "hypothesis_dash = (hypothesis_output, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n",
    "print(hypothesis_lengths[hypothesis_args_sort])\n",
    "(premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_arg_sort, hypothesis_recover_idxs) = local_inference(\n",
    "    (premise_dash, hypothesis_dash)\n",
    ")\n",
    "premise_m.size(), hypothesis_m.size(), premise_lengths.size(), hypothesis_lengths.size(), hypothesis_args_sort.size(), hypothesis_recover_idxs.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class Projection(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features,\n",
    "            out_features,\n",
    "            bias,\n",
    "    ):\n",
    "        super(Projection, self).__init__()\n",
    "        self.projection = nn.Linear(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            bias=bias\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "        premise_m = self.relu(self.projection(premise_m))\n",
    "        hypothesis_m = self.relu(self.projection(hypothesis_m))\n",
    "        return (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "class InferenceComposition(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_size=300,\n",
    "            bidirectional=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        super(InferenceComposition, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.p_dropout = p_dropout\n",
    "\n",
    "        self.premise_inference_composition = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.hypothesis_inference_composition = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=p_dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.premise_inference_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "        self.hypothesis_inference_dropout = RNNDropout(p=p_dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        (premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs) = inputs\n",
    "\n",
    "        # sort hypothesis with length\n",
    "        hypothesis_m = hypothesis_m[hypothesis_args_sort]\n",
    "        hypothesis_lengths = hypothesis_lengths[hypothesis_args_sort]\n",
    "\n",
    "        packed_premise_m = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.premise_inference_dropout(premise_m),\n",
    "            lengths=premise_lengths,\n",
    "            batch_first=True\n",
    "        )\n",
    "        packed_hypothesis_m = nn.utils.rnn.pack_padded_sequence(\n",
    "            self.hypothesis_inference_dropout(hypothesis_m),\n",
    "            lengths=hypothesis_lengths,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "        packed_premise_v, _ = self.premise_inference_composition(packed_premise_m)\n",
    "        packed_hypothesis_v, _ = self.hypothesis_inference_composition(packed_hypothesis_m)\n",
    "\n",
    "        premise_v, _ = nn.utils.rnn.pad_packed_sequence(packed_premise_v)\n",
    "        hypothesis_v, _ = nn.utils.rnn.pad_packed_sequence(packed_hypothesis_v)\n",
    "\n",
    "        premise_v_max_pooling, _ = torch.max(premise_v, dim=0)\n",
    "        hypothesis_v_max_pooling, _ = torch.max(hypothesis_v, dim=0)\n",
    "        premise_v_avg_pooling = torch.sum(premise_v, dim=0) / premise_v.size(0)\n",
    "        hypothesis_v_avg_pooling = torch.sum(hypothesis_v, dim=0) / hypothesis_v.size(0)\n",
    "\n",
    "        return torch.cat([\n",
    "            premise_v_max_pooling,\n",
    "            premise_v_avg_pooling,\n",
    "            hypothesis_v_max_pooling[hypothesis_recover_idxs],\n",
    "            hypothesis_v_avg_pooling[hypothesis_recover_idxs],\n",
    "        ], dim=1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "inference_composition = InferenceComposition(\n",
    "    input_size=4 * 300 * 2,\n",
    "    hidden_size=300,\n",
    "    bidirectional=True,\n",
    "    p_dropout=0.\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 16])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composition = inference_composition(\n",
    "    ((premise_m, premise_lengths), (hypothesis_m, hypothesis_lengths, hypothesis_args_sort, hypothesis_recover_idxs))\n",
    ")\n",
    "composition.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            num_classes,\n",
    "            hidden_sizes: list=None,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0,\n",
    "    ):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.num_classes = num_classes\n",
    "        self.act_func = act_func\n",
    "        self.bias = bias\n",
    "        self.p_dropout = p_dropout\n",
    "        self.mlp = self.get_mlp(\n",
    "            input_size=input_size,\n",
    "            hidden_sizes=hidden_sizes,\n",
    "            num_classes=num_classes,\n",
    "            act_func=act_func,\n",
    "            bias=bias,\n",
    "            p_dropout=p_dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.mlp(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mlp(input_size, hidden_sizes, num_classes, act_func, bias=True, p_dropout=0.):\n",
    "        assert num_classes > 1, 'number of classes must be more than one'\n",
    "        assert act_func in ['sigmoid', 'relu', 'tanh']\n",
    "        map_act_func = {\n",
    "            'sigmoid': nn.Sigmoid,\n",
    "            'relu': nn.ReLU,\n",
    "            'tanh': nn.Tanh,\n",
    "        }\n",
    "        sizes = [input_size, *hidden_sizes, num_classes]\n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 1):\n",
    "            if i != len(sizes) - 2:\n",
    "                layers.extend([\n",
    "                    nn.Linear(\n",
    "                        in_features=sizes[i],\n",
    "                        out_features=sizes[i + 1],\n",
    "                        bias=bias\n",
    "                    ),\n",
    "                    map_act_func[act_func](),\n",
    "                    nn.Dropout(p=p_dropout)\n",
    "                ])\n",
    "            else:\n",
    "                layers.append(nn.Linear(\n",
    "                        in_features=sizes[i],\n",
    "                        out_features=sizes[i + 1],\n",
    "                        bias=bias\n",
    "                    ))\n",
    "        return nn.Sequential(*layers)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "classifier = Classifier(\n",
    "    input_size=4 * 300 * 2,\n",
    "    num_classes=len(label_vocab.itos),\n",
    "    hidden_sizes=[512, 256],\n",
    "    act_func='sigmoid',\n",
    "    bias=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 3])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = classifier(composition)\n",
    "output.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(1.1197, grad_fn=<NllLossBackward>)"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(output, labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "class ESIM(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            word_vocab: WordVocab,\n",
    "            embedding_dim,\n",
    "            embeddings: nn.Embedding,\n",
    "            hidden_size,\n",
    "            # comp_inference_hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes: list,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0.,\n",
    "    ):\n",
    "        super(ESIM, self).__init__()\n",
    "        self.esim = self.build_model(\n",
    "            word_vocab,\n",
    "            embedding_dim,\n",
    "            embeddings,\n",
    "            hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes,\n",
    "            act_func,\n",
    "            bias,\n",
    "            p_dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, *input):\n",
    "        return self.esim(input)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_model(\n",
    "            word_vocab: WordVocab,\n",
    "            embedding_dim,\n",
    "            embeddings: nn.Embedding,\n",
    "            hidden_size,\n",
    "            bidirectional,\n",
    "            num_classes,\n",
    "            hidden_sizes: list,\n",
    "            act_func='sigmoid',\n",
    "            bias=True,\n",
    "            p_dropout=0.\n",
    "    ):\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            InputEncoding(\n",
    "                word_vocab=word_vocab,\n",
    "                embedding_dim=embedding_dim,\n",
    "                embeddings=embeddings,\n",
    "                hidden_size=hidden_size,\n",
    "                p_dropout=p_dropout\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(LocalInference())\n",
    "\n",
    "        output_local_inference_dim = (2 if bidirectional else 1) * hidden_size * 4\n",
    "\n",
    "        # layers.extend([\n",
    "        #     nn.Linear(\n",
    "        #         in_features=output_local_inference_dim,\n",
    "        #         out_features=hidden_size,\n",
    "        #         bias=bias\n",
    "        #     ),\n",
    "        #     nn.ReLU()\n",
    "        # ])\n",
    "\n",
    "        layers.append(\n",
    "            Projection(\n",
    "                in_features=output_local_inference_dim,\n",
    "                out_features=hidden_size,\n",
    "                bias=bias\n",
    "            )\n",
    "        )\n",
    "\n",
    "        layers.append(\n",
    "            InferenceComposition(\n",
    "                input_size=hidden_size,\n",
    "                hidden_size=hidden_size,\n",
    "                bidirectional=bidirectional,\n",
    "                p_dropout=p_dropout\n",
    "            )\n",
    "        )\n",
    "        output_inference_composition_dim = (2 if bidirectional else 1) * hidden_size * 4\n",
    "        layers.extend(\n",
    "            [\n",
    "                nn.Dropout(p=p_dropout),\n",
    "                Classifier(\n",
    "                    input_size=output_inference_composition_dim,\n",
    "                    hidden_sizes=hidden_sizes,\n",
    "                    num_classes=num_classes,\n",
    "                    act_func=act_func,\n",
    "                    bias=bias\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        return nn.Sequential(*layers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__( self, *args, **kwargs):\n",
    "        for k, v in self.default_config().items():\n",
    "            self.__setattr__(k, v)\n",
    "        for k, v in kwargs.items():\n",
    "            self.__setattr__(k, v)\n",
    "\n",
    "        assert self.__getattribute__('word_vocab') is not None\n",
    "        assert self.__getattribute__('num_classes') is not None\n",
    "\n",
    "    @staticmethod\n",
    "    def default_config():\n",
    "        hidden_size = 300\n",
    "        return {\n",
    "            'word_vocab': None,\n",
    "            'embedding_dim': 300,\n",
    "            'embeddings': None,\n",
    "            'hidden_size': hidden_size,\n",
    "            'bidirectional': True,\n",
    "            'num_classes': None,\n",
    "            'hidden_sizes': [hidden_size],\n",
    "            'act_func': 'tanh',\n",
    "            'bias': True,\n",
    "            'p_dropout': 0.5,\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    word_vocab=word_vocab,\n",
    "    num_classes=len(label_vocab.itos)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhlt998/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = ESIM(\n",
    "    word_vocab=config.word_vocab,\n",
    "    embedding_dim=config.embedding_dim,\n",
    "    embeddings=config.embeddings,\n",
    "    hidden_size=config.hidden_size,\n",
    "    bidirectional=config.bidirectional,\n",
    "    num_classes=config.num_classes,\n",
    "    hidden_sizes=config.hidden_sizes,\n",
    "    act_func=config.act_func,\n",
    "    bias=config.bias,\n",
    "    p_dropout=config.p_dropout\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([32, 3])"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(premise, (hypothesis, hypothesis_args_sort, hypothesis_recover_idxs))\n",
    "output.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "learner = Learner(\n",
    "    data=databunch,\n",
    "    model=model,\n",
    "    loss_func=loss_func,\n",
    "    path='.',\n",
    "    metrics=[accuracy]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n      \n    </div>\n    \n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 458.00 MiB (GPU 0; 3.95 GiB total capacity; 2.82 GiB already allocated; 248.62 MiB free; 459.89 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-116-74ea54b65c71>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mlearner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr_find\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mlearner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecorder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/train.py\u001B[0m in \u001B[0;36mlr_find\u001B[0;34m(learn, start_lr, end_lr, num_it, stop_div, wd)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0mcb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mLRFinder\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mend_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_it\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstop_div\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0mepochs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mceil\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_it\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 41\u001B[0;31m     \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstart_lr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     42\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m def to_fp16(learn:Learner, loss_scale:float=None, max_noskip:int=1000, dynamic:bool=True, clip:float=None,\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, epochs, lr, wd, callbacks)\u001B[0m\n\u001B[1;32m    198\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mwd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    199\u001B[0m         \u001B[0mcallbacks\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mcb\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcb\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallback_fns\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlistify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdefaults\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextra_callback_fns\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mlistify\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 200\u001B[0;31m         \u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetrics\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetrics\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0mcallbacks\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    201\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    202\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mcreate_opt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mFloats\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwd\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mFloats\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m->\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(epochs, learn, callbacks, metrics)\u001B[0m\n\u001B[1;32m     99\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0myb\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mprogress_bar\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_dl\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mpbar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    100\u001B[0m                 \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 101\u001B[0;31m                 \u001B[0mloss\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mxb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0myb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloss_func\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    102\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_batch_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/fastai/basic_train.py\u001B[0m in \u001B[0;36mloss_batch\u001B[0;34m(model, xb, yb, loss_func, opt, cb_handler)\u001B[0m\n\u001B[1;32m     32\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mopt\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0mloss\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mskip_bwd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_backward_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 34\u001B[0;31m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mskip_bwd\u001B[0m\u001B[0;34m:\u001B[0m                     \u001B[0mloss\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     35\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_backward_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mcb_handler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_step_end\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m     \u001B[0mopt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(self, gradient, retain_graph, create_graph)\u001B[0m\n\u001B[1;32m    164\u001B[0m                 \u001B[0mproducts\u001B[0m\u001B[0;34m.\u001B[0m \u001B[0mDefaults\u001B[0m \u001B[0mto\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;31m`\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    165\u001B[0m         \"\"\"\n\u001B[0;32m--> 166\u001B[0;31m         \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    168\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/deep-learning/lib/python3.6/site-packages/torch/autograd/__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001B[0m\n\u001B[1;32m     97\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[1;32m     98\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgrad_tensors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 99\u001B[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001B[0m\u001B[1;32m    100\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 458.00 MiB (GPU 0; 3.95 GiB total capacity; 2.82 GiB already allocated; 248.62 MiB free; 459.89 MiB cached)"
     ]
    }
   ],
   "source": [
    "learner.lr_find()\n",
    "learner.recorder.plot()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learner.fit(\n",
    "    epochs=15,\n",
    "    callbacks=[\n",
    "      SaveModelCallback(learner, monitor='accuracy', every='improvement', name='best_model'),\n",
    "      EarlyStoppingCallback(learner, monitor='valid_loss', min_delta=0.01, patience=10),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "3496"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.load('best_model')\n",
    "learner.evaluate(dl=learner.data.test_dl)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}